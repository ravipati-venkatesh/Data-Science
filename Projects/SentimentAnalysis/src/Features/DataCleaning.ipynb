{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab043330",
   "metadata": {},
   "source": [
    "# In this model I will be using glove embeddings.It has a large vocabulary and we can find the words from our data which are not present in the glove( these words are contractions, misspelled words, concated words or emojis which can decrease our model's performance. We will then use re library to remove these words from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99597967",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3993239a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path for the processed data\n",
    "processed_file_path = \"../../Data/Process/sample_preprocessed_data.xlsx\"\n",
    "\n",
    "# Load the preprocessed data\n",
    "df = pd.read_excel(processed_file_path)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save the training set to an Excel file\n",
    "train_file_path = \"../../Data/Process/sample_train_data.xlsx\"\n",
    "train_df.to_excel(train_file_path, index=False)\n",
    "\n",
    "# Save the testing set to an Excel file\n",
    "test_file_path = \"../../Data/Process/sample_test_data.xlsx\"\n",
    "test_df.to_excel(test_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14c10b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_build(review):\n",
    "    \n",
    "    comments = review.apply(lambda s: s.split()).values\n",
    "    vocab={}\n",
    "    \n",
    "    for comment in comments:\n",
    "        for word in comment:\n",
    "            try:\n",
    "                vocab[word]+=1\n",
    "                \n",
    "            except KeyError:\n",
    "                vocab[word]=1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d40905a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_coverage(review,embeddings):\n",
    "    \n",
    "    vocab=vocab_build(review)\n",
    "    \n",
    "    covered={}\n",
    "    word_count={}\n",
    "    oov={}\n",
    "    covered_num=0\n",
    "    oov_num=0\n",
    "    \n",
    "    for word in vocab:\n",
    "        try:\n",
    "            covered[word]=embeddings[word]\n",
    "            covered_num+=vocab[word]\n",
    "            word_count[word]=vocab[word]\n",
    "        except:\n",
    "            oov[word]=vocab[word]\n",
    "            oov_num+=oov[word]\n",
    "    \n",
    "    vocab_coverage=len(covered)/len(vocab)*100\n",
    "    text_coverage = covered_num/(covered_num+oov_num)*100\n",
    "    \n",
    "    sorted_oov=sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
    "    sorted_word_count=sorted(word_count.items(), key=operator.itemgetter(1))[::-1]\n",
    "    \n",
    "    return sorted_word_count,sorted_oov,vocab_coverage,text_coverage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de976e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Define contraction replacements\n",
    "    contractions = {\n",
    "        \"isnt\": \"is not\",\n",
    "        \"arent\": \"are not\",\n",
    "        \"wasnt\": \"was not\",\n",
    "        \"werent\": \"were not\",\n",
    "        \"hasnt\": \"has not\",\n",
    "        \"havent\": \"have not\",\n",
    "        \"hadnt\": \"had not\",\n",
    "        \"doesnt\": \"does not\",\n",
    "        \"dont\": \"do not\",\n",
    "        \"didnt\": \"did not\",\n",
    "        \"wont\": \"will not\",\n",
    "        \"wouldnt\": \"would not\",\n",
    "        \"shant\": \"shall not\",\n",
    "        \"shouldnt\": \"should not\",\n",
    "        \"cant\": \"cannot\",\n",
    "        \"couldnt\": \"could not\",\n",
    "        \"mightnt\": \"might not\",\n",
    "        \"mustnt\": \"must not\",\n",
    "        \"lowbudget\": \"low budget\",\n",
    "        \"overthetop\": \"over the top\",\n",
    "        \"filmThe\": \"film the\",\n",
    "        \"movieThe\": \"movie the\",\n",
    "        \"realise\": \"realize\",\n",
    "        \"mustsee\": \"must see\",\n",
    "        \"aint\": \"am not\"\n",
    "    }\n",
    "    \n",
    "    # Replace contractions\n",
    "    for contraction, replacement in contractions.items():\n",
    "        text = re.sub(r\"\\b{}\\b\".format(contraction), replacement, text, flags=re.IGNORECASE)\n",
    "    \n",
    "\n",
    "    text = re.sub(r\"(\\b\\w+)n't\\b\", r\"\\1 not\", text)\n",
    "\n",
    "    # Remove punctuation at the end of words\n",
    "    text = re.sub(r\"(\\b\\w+)[.,]\", r\"\\1\", text)\n",
    "    \n",
    "    # Remove any other unwanted characters\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "\n",
    "\n",
    "    # Remove unwanted characters like -- and -\n",
    "    text = re.sub(r\"--\", \" \", text)\n",
    "    text = re.sub(r\"-\", \" \", text)\n",
    "\n",
    "    # Clean up punctuation\n",
    "    text = re.sub(r\"\\s([.,!?;:])\", r\"\\1\", text)  # Remove space before punctuation\n",
    "    text = re.sub(r\"[.,!?;:](?=\\w)\", r\"\\g<0> \", text)  # Add space after punctuation if followed by a word\n",
    "    text = re.sub(r\"[.,!?;:]{2,}\", r\"\", text)  # Remove duplicate punctuation\n",
    "\n",
    "    # Additional cleanup\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s.,!?;:]\", \"\", text)  # Remove any other special characters\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Replace multiple spaces with a single space\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5fe317",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "# Load the GloVe embeddings\n",
    "word2vec_model = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f211e72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\ravip\\\\Documents\\\\Data-Science\\\\Projects\\\\SentimentAnalysis\\\\src\\\\Features'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = train_df['review']\n",
    "train_covered,train_oov,train_vocab_coverage,train_text_coverage=embedding_coverage(X_train,word2vec_model)\n",
    "# test_covered,test_oov, test_vocab_coverage, test_text_coverage = embedding_coverage(X_test,glove_embeddings)\n",
    "\n",
    "print(f\"word2vec embeddings cover {round(train_vocab_coverage,2)}% of vocabulary and {round(train_text_coverage,2)}% text in training set\")\n",
    "\n",
    "train_df['review']=train_df['review'].apply(lambda s: preprocess_text(s))\n",
    "test_df['review']=test_df['review'].apply(lambda s: preprocess_text(s))\n",
    "\n",
    "train_covered,train_oov,train_vocab_coverage,train_text_coverage=embedding_coverage(train_df['review'],word2vec_model)\n",
    "print(f\"Glove embeddings cover {round(train_vocab_coverage,2)}% of vocabulary and {round(train_text_coverage,2)}% text in training set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8e16f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the training set to an Excel file\n",
    "train_file_path = \"../../Data/Process/sample_train_data.xlsx\"\n",
    "train_df.to_excel(train_file_path, index=False)\n",
    "\n",
    "# Save the testing set to an Excel file\n",
    "test_file_path = \"../../Data/Process/sample_test_data.xlsx\"\n",
    "test_df.to_excel(test_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a83945",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
